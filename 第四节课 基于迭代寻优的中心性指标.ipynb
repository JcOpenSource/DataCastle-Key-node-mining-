{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于迭代寻优的中心性指标，这类方法理论简洁而优美，应用广泛且有效（讲者十分推荐这类方法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代寻优的基础思路\n",
    "一个节点的【重要性/质量/影响力】\n",
    "\n",
    "$\\qquad$决定于\n",
    "\n",
    "$\\qquad$$\\qquad$它的__邻居的__【重要性/质量/影响力】\n",
    "\n",
    "具体到每个算法，不同算法的不同点在于__邻居节点的作用方式__不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路：一个节点的重要性是取决于它邻居的重要性。简单来讲如果一个节点的邻居比较重要，那么这个节点也就比较重要。具体来讲，邻居的重要性在多大程度上影响目标节点的重要性是这类算法的主要的不同点。下面介绍几个经典的指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.特征向量中心性\n",
    "一个节点的中心性__正比于__它的邻居的中心性之和\n",
    "$$x_{i} = c \\sum_{n}^{j=1}a_{ij}x_{j}$$\n",
    "* $a_{ij}$是__邻接矩阵__A的第$i$行第$j$列的元素\n",
    "* $c = 1/ \\lambda$，而$\\lambda$是邻接矩阵的最大特征值\n",
    "* $x_{i}$是节点i的中心性，$x_{j}$是节点j的中心性，\n",
    "* 公式的核心点：$a_{ij}x_{j}$如果节点i到节点j有连边的话$A_{ij}$就等于1，如果没有的话就是0.这个公式也就是讲对于节点i，我去观察他的邻居，然后把它邻居的中心性相加起来。就是目标节点i的中心性。很显然这是一个迭代的定义了。\n",
    "* 正面来讲绩点j的中心性会影响节点i的中心性，反过来讲节点i也会影响节点j的中心性。\n",
    "\n",
    "\n",
    "__该式子还可以写出矩阵的形式__\n",
    "\n",
    "$$\\vec{x}= cA \\vec{x}$$\n",
    "<img src='fig\\4-2.png' width='200' height='20'>\n",
    "文献：\n",
    "<img src='fig\\4-1.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征向量中心性的计算：\n",
    "* 1.初始化，每个节点的特征向量的中心性都为1.\n",
    "* 2.之后根据公式$x_{i} = c \\sum_{n}^{j=1}a_{ij}x_{j}$来计算每一个节点下一步的中心性。\n",
    "    * 例如：计算节点1的中心性：节点1有三个邻居节点2，4，3\n",
    "    $x_1 = c * （1+1+1）=3 c$ c为系数\n",
    "  依次类推计算出x2，x3，x4的中心性。\n",
    "* 3.接着进行下一步的迭代，依据上一步迭代出的值。计算中心性\n",
    "* 4.如此王府不断计算下去就可以得到一个收敛的值。因为这里的c就是为了去控制迭代的收敛性而设置的。它的值就是邻接矩阵的最大特征值的倒数。\n",
    "\n",
    "这个式子可以进一步写出一个矩阵的形式$$\\vec{x}= cA \\vec{x}$$\n",
    "* c：系数\n",
    "* A：邻接矩阵\n",
    "* $\\vec{x}$:节点中心性的向量\n",
    "这就式一个矩阵的表达形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征向量中心性的问题\n",
    "# 大度节点会显著的自我加强（基于特征向量中心性的缺点一）\n",
    "\n",
    "解决方法\n",
    "\n",
    "$\\qquad$ non-backtracking（无回溯）矩阵\n",
    "$$x_{j} = \\sum_{i}A_{ij}v_{i  \\rightarrow j}$$\n",
    "\n",
    "$v_{i\\rightarrow j}$可以控制节点自身影响的回传，通过无回溯矩阵定义\n",
    "<img src='fig\\4-3.png' width='200' height='20'>\n",
    "文献：\n",
    "<img src='fig\\4-1.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：特征向量中心性的计算方法是存在一定问题的，为什么呢？是因为大度节点会显著的自我加强。图中紫色节点是一个度比较大的节点。比如说在上一步它把自己的，从其他度节点收集过来的中心性比较大的值，又会分散给其他的邻居节点。那么在以后的部署中，这些被分散回去的中心性的值，又会返回来去加强这个节点自身。所以这种方法得到的结果就是中心性的值，会集中在较少数的几个度大的节点上面。为了解决这一个问题，其实思路也很简单。（但首先提出还是很厉害的）。Mark Newman（复杂网络大牛🐮）的组。提出的方法 利用 no-backtracking matrix 无回溯矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无回溯矩阵：\n",
    "* 简单思路理解：它就是杜绝刚才所说过的这个节点，节点把它的中心性传出去然后在返回来加强自己，杜绝这种情况的产生。\n",
    "* 方法的定义：与前面特征向量中心性类似，有一个邻接矩阵的元素$A_{ij}$,i到j是否有连接\n",
    "    * $v_{i  \\rightarrow j}$：non-backtracking的精髓所在，它控制节点自身影响的回传的，它的定义是通过无回溯矩阵来定义的。\n",
    "### 什么是无回溯矩阵：（见下页ppt）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无回溯矩阵（基于特征向量中心性的缺点一的解决方法）\n",
    "无回溯矩阵的定义分为两步：\n",
    "\n",
    "初始条件：对n个节点、m条边的__无向网路__\n",
    "* 1.先将它变为2m条边的有向网路\n",
    "* 2.构建$2m\\times2m$的无回溯矩阵__B__\n",
    "\n",
    "具体做法：把一个无向边拆成两条，就变成了2m条边的有向网络。\n",
    "$$将一条边 - 拆分成  \\leftarrow 和 \\rightarrow $$\n",
    "然后在根据这个网络去构建无回朔矩阵，无回溯矩阵的大小为$2m\\times2m$，一共有2m条边，所以每一个矩阵中的每一个元素，其实就是每一对边他们之间的关系。\n",
    "\n",
    "举例子：原始网络有一条边是1到2，和一条边1到3，则它的无回溯矩阵是\n",
    "\n",
    "\\begin{align}\n",
    "  \\begin{matrix}\n",
    "       & 1 \\leftarrow 2       & 2 \\leftarrow 1      & 1 \\leftarrow 3      & 3 \\leftarrow 1  \\\\\n",
    "1 \\leftarrow 2      & *          & *        & *      & *  \\\\\n",
    "2 \\leftarrow 1      &   *        &  *       &  *     & *  \\\\\n",
    "1 \\leftarrow 3      &  *         &  *       & *      &  * \\\\\n",
    "3 \\leftarrow 1      &  *         &  *       &   *    &  * \\\\\n",
    "\\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "邻接矩阵每一行没一列的元素都代表一个点，无回朔矩阵的每一行每一列的元素表示一个边（有向）\n",
    "$\\qquad$每一行/列对应到一条边\n",
    "\n",
    "## 无回溯矩阵里面元素的定义：\n",
    "\n",
    "$\\qquad$其中元素定义为：\n",
    "$$B_{k \\rightarrow l,i \\rightarrow j} = \\delta_{jk}(1-\\delta_{ij})$$\n",
    "* 无回溯矩阵的元素下标代表：边$k \\rightarrow l$ 和 边$ i \\rightarrow j$ \n",
    "* $\\wp_{jk}$函数的意义：\n",
    "    * 如果 j=k , $\\delta_{jk} = 1$\n",
    "    * 否值$\\delta_{jk} = 0$\n",
    "    * 即$$ \\delta_{jk}=\\left\\{\n",
    "\\begin{aligned}\n",
    "1 & & i=j \\\\\n",
    "0 & & i \\not= j\\\\\n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "\n",
    "\n",
    "<img src='fig\\4-4.png' width='300'>\n",
    "\n",
    "有上图可以看出$\\delta_{jk}$ 和$ \\delta_{ij}$除非他们两个不全相等，如果是j等于k 的话，i 就一定不能等于l，如果是i等于l 的话，j 就一定不能等于k。也就是它要去保证这两条连边的节点不能合并成这样的一种状态，如图所示。\n",
    "<img src='fig\\1.jpg' width='80'>\n",
    "从此，我们就能够去保证一个点的影响，不能直接被回传回来。这个就是无回溯矩阵的一个定义方法。就是通过对比每两条边他们之间的异同来决定一个节点外传的能量能不能回传回来，由此来控制的。\n",
    "\n",
    "然后$v_{i \\rightarrow j}$是无回溯矩阵B的leading EifenVector的元素（这个是最大特征值对应的特征向量）\n",
    "\n",
    "这种方法的复杂性比较高，本来把一个$n*n$的矩阵，m一般都是n的好多倍。然后把它变成一个$2m*2m$的矩阵，再去求它的特征向量，特征值，这本身就是一个非常复杂的过程，因此在非常大的网络里面可能并不是很使用，除非对算法进行较大的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征向量中心性 vs nonbacktracking\n",
    "<img src='fig\\4-5.png' width='500'>\n",
    "文献：\n",
    "<img src='fig\\4-1.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征向量中心性和基于无回溯矩阵的这个中心性指标，它们在应用上的差异。\n",
    "\n",
    "从上图可以看出，这两个是同一个网络，只是节点的大小不一样，在这里节点的大小就是代表中心性值的大小，左图是特征向量中心性的大小，右边的图则是基于无回溯矩阵的中心性大小。在左图里面中心性比较大的这些点就非常小，就只集中在中心的这些节点里面。而当我们杜绝了节点自生能量回传的时候，我们就可以把更多的节点的中心性得到很好的区分，那么就是基于无回溯矩阵的中心性指标，这个效果是更好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征向量中心性收敛速度慢（基于特征向量中心性的缺点二 ）\n",
    "# 累计提名（基于特征向量中心性的缺点二 de解决办法）\n",
    "\n",
    "累计提名算法加强自己对自己的提名\n",
    "\n",
    "累计提名于特征向量中心性迭代方法类似\n",
    "\n",
    "但为解决特征向量中心性__收敛速度慢__的问题，加强了自己对自己的提名\n",
    "$$\\widetilde{p}_{i}(t) = \\widetilde{p}_{i}(t-1) + \\sum_{j}a_{ij}\\widetilde{p}_{j}(t-1)$$\n",
    "为了保证收敛，需要对累计提名中心性进行归一处理\n",
    "$$p_{i}(t) = \\frac{\\widetilde{p}_{i}(t)}{\\sum_{j}\\widetilde{p}_{j}(t)}$$\n",
    "文献：\n",
    "<img src='fig\\4-6.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记录：不考虑中间的那一块，$\\widetilde{p}_{i}(t) = \\sum_{j}a_{ij}\\widetilde{p}_{j}(t-1)$与$x_{i} = c \\sum_{n}^{j=1}a_{ij}x_{j}$很相似，区别仅仅少了系数c。$ \\widetilde{p}_{i}(t-1)$上一步自身的中心性。也就是利用了自己对自己的提名，跟非回溯矩阵反向而行。它的优点是收敛比较快，但是为了保证收敛，需要对累计提名中心性每一步迭代之后，做一个归一化的处理才能继续的往后迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pageRank\n",
    "是特征向量中心性的变种，它是提出是为了对__网页质量__进行排名 (Google初期核心算法)\n",
    "\n",
    "基本思想：一个网页越重要，会被更多重要的网页建立链接\n",
    "\n",
    "网页中的随机游走！\n",
    "\n",
    "文献：\n",
    "<img src='fig\\4-7.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记录：从根上将是pageRank特征向量中心性的变种，起初它是提出是为了对网页质量进行排名 ，它也是Google初期核心算法，凭借该算法使得它拿下了搜索市场巨大的份额。\n",
    "\n",
    "介绍pageRank先从网络中的随机游走开始讲起。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 随机游走\n",
    "<img src='fig\\4-8.png' width='300'>\n",
    "在$t_0$时刻从$s$出发，在$t_3$时刻到达网页$u$的概率\n",
    "$$p_{u}(t_{3}) = \\frac{p_{a}(t_{2})}{3} + \\frac{p_{b}(t_{2})}{2} + \\frac{p_{c}(t_{2})}{3}$$\n",
    "$$\\Downarrow$$\n",
    "$$ p_{u}(t_{3}) = \\frac{p_{a}(t_{2})}{k_{a}^{out}} + \\frac{p_{b}(t_{2})}{k_{b}^{out}} + \\frac{p_{c}(t_{2})}{k_{c}^{out}} $$\n",
    "$$\\Downarrow$$\n",
    "$$p_{i}(t_{n}) = \\sum_{j=1}^{N}\\frac{a_{ij}}{k_{j}^{out}}p_{j}(t_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看出t2时刻到达a，a的下一步方向有3个，选中u的概率是$\\frac{1}{3}$\n",
    "\n",
    "$k_{j}^{out}$:节点j的出度。\n",
    "\n",
    "公式的含义：一个用户在tn时刻停留在i页面的概率就等于，在tn-1时刻在所有能够到达它的节点j它的概率，除以它的出度，然后在求和，__【随机游走在网络中的核心思想】__\n",
    "\n",
    "从公式可以看出它是一个迭代的公式，一般对于无向连通网络来讲这个式子是能收敛的。也有不能收敛的情况。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记录：\n",
    "* 案例：\n",
    "    * 走迷宫：假如在一个迷宫中，不知道任何规则，所以只能乱走，随机走到下一个地点在随机走到下下一个地点。（这也就是随机游走的一个简单解释）\n",
    "    * 网上冲浪：假如我们浏览网页并不是随机的，而是基于网页之间的超链接进行访问，我们在平时的时候也是，要去搜索某个页面，某个内容，我们要先打开百度，然后键入关键词，百度回给我一系列连接，此时我们则会根据百度与这些链接建立的关系来从而来访问我们想要的目标页面，这是网页里面也有超链接这样我们就从一个超链接到下一个超链接，\n",
    "* 也就是讲如果一个页面被多个页面所链接，说明这个节点是比较重要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从随机游走的过程到PageRank算法的设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 从随机游走到PageRank\n",
    "当某个页面再无外链，就掉入陷阱而无法继续游走\n",
    "\n",
    "PageRank引入随机跳转\n",
    "$$p_i = \\frac{1-c}{N} + c \\cdot \\sum_{j=1}^{N}\\frac{a_{ji}}{k_j^{out}} \\cdot p_j$$\n",
    "$$\\Downarrow$$\n",
    "$$p_i(t_n) = \\frac{1-c}{N} +  c \\cdot \\sum_{j=1}^{N}\\frac{a_{ji}}{k_j^{out}}p_j(t_{n-1})$$\n",
    "<img src='fig\\4-10.png' width='500'> \n",
    "文献：\n",
    "<img src='fig\\4-9.png' width='500'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从随机游走到pagerank的关键一步：一个用户在某一个时刻，在某一个页面停留的概率越大也就说明它越重要呢，\n",
    "\n",
    "到达网络中某一个节点，没有路再可以走，（节点无出度）即网页中的陷阱，如果到达这样的节点网络的整个迭代则毫无意义，PageRank是如何解决这个问题的呢：通常随机游走是，我们每一步的访问都要基于网页之间的超链接，那么我们平常在访问网页的时候不会这样去做，我们有一定的概率去跳转到其他页面，就比如说我有一定的概率跳转回百度，那也有一定的概率跳回到某一个网站，（这都是不确定的）这个时候我们的跳转就不再是基于网页之间的超链接了，而是一种人类的行为。那么这个时候，PageRank里面引入了一个这样的元素\n",
    "\n",
    "\n",
    "$\\frac{1-c}{N}$  PageRank里面引入了一个元素，N网络中节点的数目，1-c表示一个概率，c通常去0.85，1-c表示它有15%的可能性跳转到其他页面，这都是一个经验值，引入随机跳转之后，我们就可以解决这样的陷阱问题，，应为即便是我到达可无出度的节点页面，我仍然返回到其他页面，而不会陷入一个僵局。\n",
    "\n",
    "这个式子也可以写成我们常见的基于时间迭代的方法：$p_i(t_n) = \\frac{1-c}{N} +  c \\cdot \\sum_{j=1}^{N}\\frac{a_{ji}}{k_j^{out}}p_j(t_{n-1})$\n",
    "\n",
    "PageRank是通过引入随机跳转来解决网页陷阱问题。\n",
    "\n",
    "还有一个方法它更加巧妙的解决了跳转问题，而且没有引入的参数。不用参数来解决这个问题就是LeaderRank算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 从随机游走到LeaderRank\n",
    "为了解决陷阱问题。引入__超节点__\n",
    "<img src='fig\\4-11.png' width='300'> \n",
    "$$s_{i}(t_{n}) = \\sum_{j=1}^{N+1} \\frac{a_{ji}}{k_{j}^{out}}s_j(t_{n-1})$$\n",
    "$$\\Downarrow$$\n",
    "$$s_i = s_i(t_c) + \\frac{s_g(t_c)}{N}$$\n",
    "文献：\n",
    "<img src='fig\\4-12.png' width='500'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决陷阱问题。LeaderRank算法引入__超节点__。这个超节点跟网络中所有的节点（用户）都建立起了一个双向的连边，这样的话就不回有节点在是陷阱了。因为它总可以通过这个超节点最后再返回到其他节点，公式就是随机游走的式子，算法最终会收敛，因为grand node 的存在。\n",
    "\n",
    "最终每个节点Leaderank值就是超节点最后收敛的值$s_g(t_c)$ 加上 每个节点最后收敛的值$s_i(t_c)$.即laederrank最后的中心性指标的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几乎与pageRank同期还有一个比较著名的算法，叫HITS算法。这个指标是为网页的质量专门定义的。与pagerank不同的是他把网页的属性分成两个，一个是Hub属性，我们称为中心性；一个是authority属性，我们称为权威性。基本假设一个高质量的authority页面会被很多高质量的hub页面所指向。反正一个高质量的hub页面会指向很多高质量的authority页面。例如百度，百度就可以看成为一个Hub页面，他有很多指向其他页面的超链。如果有很多这样的Hub，他们都把链接或者比较靠前的链接指向某一个页面的话，这个页面质量也就相对叫高，其实这个页面就是我们所说的权威页面。也就是这个页面的权威性较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HITS算法 - Hyperlink-Induced Topic Search\n",
    "网页的hub属性和authority属性\n",
    "\n",
    "* 一个高质量的authority页面会被很多高质量的hub页面所指向。\n",
    "* 一个高质量的hub页面会指向很多高质量的authority页面\n",
    "<img src='fig\\4-13.png' width='500'> \n",
    "$$a'_i(t) = \\sum_{j=1}^{n}a_{ji}h_j(t-1)$$\n",
    "\n",
    "$$h'_i(t) = \\sum_{j=1}^{n}a_{ij}a'_j(t)$$\n",
    "文献：\n",
    "<img src='fig\\4-14.png' width='500'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "节点的authority值：$a'_i(t) = \\sum_{j=1}^{n}a_{ji}h_j(t-1)$\n",
    "\n",
    "一个页面的authority值是由一个页面（节点）的Hub值所决定的，$a_{ji}$通过邻接矩阵找到指向页面（节点）i的页面（节点）j。再去把页面（节点）j的Hub值加起来，就是目标页面i的authority值\n",
    "\n",
    "\n",
    "\n",
    "节点的Hub值：$h'_i(t) = \\sum_{j=1}^{n}a_{ij}a'_j(t)$\n",
    "\n",
    "页面i的Hub值，从邻接矩阵入手，从网络中寻找所有目标页面指向的节点，i->j，目标节点i指向了节点j，把这些页面j的authority值相加就是页面i的hub值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算：\n",
    "* 迭代的定义，先初始化，每个节点的hub属性都是1，记为T0时刻。\n",
    "* 先根据T0时刻Hub值，计算T1时刻authority值。\n",
    "* 再更加T1时刻authority值，计算T1时刻Hub值.\n",
    "* 每一次迭代之后，对这些值进行归一化处理。（可以看出迭代之后，值都是会变大的，很难收敛）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "更多扩展指标\n",
    "请参考（更多中心性指标的变体）\n",
    "<img src='fig\\4-15.png' width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
